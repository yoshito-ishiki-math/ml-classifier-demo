import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier



#########################
##å‰å‡¦ç†
#########################



# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
df['is_input'] = False

#########################
##å‰å‡¦ç†
#########################



#######
###ã‚µã‚¤ãƒ‰ãƒãƒ¼
######
#ã‚µã‚¤ãƒ‰ãƒãƒ¼ --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ---
st.sidebar.header("ğŸ§ª ç‰¹å¾´é‡ã®å…¥åŠ›")
st.sidebar.write("ã‚ãªãŸã®èŠ±ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å…¥åŠ›ã—ã¦ã­")
user_input = {
    name: st.sidebar.slider(name, float(df[name].min()), float(df[name].max()), float(df[name].mean()))
    for name in iris.feature_names
}
input_df = pd.DataFrame([user_input])
input_df['species'] = 'ã‚ãªãŸã®å…¥åŠ›'
input_df['is_input'] = True

# --- çµåˆ ---
combined_df = pd.concat([df, input_df], ignore_index=True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼--- è»¸ã®é¸æŠ ---
st.sidebar.header("ğŸ§­ è¡¨ç¤ºè»¸ã®é¸æŠ")
x_axis = st.sidebar.selectbox("Xè»¸", iris.feature_names, index=2)
y_axis = st.sidebar.selectbox("Yè»¸", iris.feature_names, index=3)


####
# --- ã‚°ãƒ©ãƒ•æç”» ---
####
fig = go.Figure()

# å“ç¨®ã”ã¨ã«è‰²ã‚’å‰²ã‚Šå½“ã¦
color_map = {
    "setosa": "blue",
    "versicolor": "green",
    "virginica": "orange"
}

# Iris ãƒ‡ãƒ¼ã‚¿ã‚’ species ã”ã¨ã«æç”»
for species_name, color in color_map.items():
    species_df = combined_df[(combined_df["species"] == species_name) & (combined_df["is_input"] == False)]
    fig.add_trace(go.Scatter(
        x=species_df[x_axis],
        y=species_df[y_axis],
        mode='markers',
        name=species_name,
        marker=dict(size=4, color=color, opacity=0.6),
        showlegend=True
    ))

# ã‚ãªãŸã®å…¥åŠ›ï¼ˆèµ¤ãã¦å¤§ããã¦ç›®ç«‹ã¤ï¼‰
input_only_df = combined_df[combined_df["is_input"] == True]
fig.add_trace(go.Scatter(
    x=input_only_df[x_axis],
    y=input_only_df[y_axis],
    mode='markers+text',
    name='ã‚ãªãŸã®å…¥åŠ›',
    marker=dict(size=12, color='red', symbol='x', line=dict(width=2, color='DarkRed')),
    text=["ã‚ãªãŸã®å…¥åŠ›"],
    textposition='top center',
    showlegend=True
))

# ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
fig.update_layout(
    title=f"{x_axis} vs {y_axis} with Your Input",
    xaxis_title=x_axis,
    yaxis_title=y_axis
)


st.sidebar.subheader("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒƒãƒˆã¯äºŒã¤ã®ç‰¹å¾´æ–™ã®ãƒ—ãƒ­ãƒƒãƒˆã¨ã‚ãªãŸã®å…¥åŠ›ã®ä½ç½®ã§ã™")
# Streamlitã§è¡¨ç¤º
st.sidebar.plotly_chart(fig, use_container_width=True)
####
# --- ã‚°ãƒ©ãƒ•æç”» ---
####






#######
###ã‚µã‚¤ãƒ‰ãƒãƒ¼
######



#######################
##æ‰‹æ³•ãªã©ã®è¨˜è¿°
####################

###return ã«ã¯pred_classã¨proba_dictã‚’å…¥ã‚Œã‚‹ï¼

# --- K-NN ç”¨ã®é–¢æ•° ---
def apply_k_nn_method(params):
    model = KNeighborsClassifier(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)
    
    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]
    
    proba = model.predict_proba(input_data)[0]  # â† ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆ1æ¬¡å…ƒï¼‰
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }
    return pred_class, proba_dict



#--æ±ºå®šæœ¨
def apply_decision_tree_method(params):
    model = DecisionTreeClassifier(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict


#--ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚º
def apply_naive_bayes_method(params):
    model = GaussianNB()
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict


##ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
def apply_random_forest_method(params):
    model = RandomForestClassifier(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict


###SVC(probability=True)
def apply_svm_method(params):
    model = SVC(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict



###ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
def apply_logistic_regression_method(params):
    model = LogisticRegression(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict



#å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°
def apply_gradient_boosting_method(params):
    model = GradientBoostingClassifier(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict


##ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ(MLP)
def apply_mlp_method(params):
    model = MLPClassifier(**params)

    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict



##XGBoost

def apply_xgboost_method(params):
    model = XGBClassifier(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]
    
    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict


#LightGBM
def apply_lightgbm_method(params):
    model = LGBMClassifier(**params)
    feature_matrix = df[iris.feature_names]
    species_labels = iris.target
    input_data = input_df[iris.feature_names]

    model.fit(feature_matrix, species_labels)

    prediction = model.predict(input_data)
    pred_class = iris.target_names[prediction[0]]

    proba = model.predict_proba(input_data)[0]
    proba_dict = {
        name: f"{p:.2%}" for name, p in zip(iris.target_names, proba)
    }

    return pred_class, proba_dict



####
#æ‰‹æ³•ã®è¾æ›¸
####

CLASSIFYING_MACHINES={
    "k-NN":apply_k_nn_method,
    "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°":apply_logistic_regression_method,
    "ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚º":apply_naive_bayes_method,
    "ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³":apply_svm_method,
    "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ(MLP)":apply_mlp_method,
    "æ±ºå®šæœ¨":apply_decision_tree_method,
    "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ":apply_random_forest_method,
    "å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°":apply_gradient_boosting_method,
    "XGBoost":apply_xgboost_method,
    "LightGBM":apply_lightgbm_method
    }



###########
##ãƒã‚¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIã®å®šç¾©
##########
def ui_k_nn_params():
    return {
        "n_neighbors": st.slider("è¿‘å‚æ•°(k)", 1, 15, 5)
    }

def ui_decision_tree_params():
    return {
        "max_depth": st.slider("æœ¨ã®æ·±ã•(max_depth)", 1, 10, 3),
        "min_samples_split": st.slider("ãƒãƒ¼ãƒ‰ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæœ€å°ã®ã‚µãƒ³ãƒ—ãƒ«æ•°(min_samples_split)", 2, 10, 2),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰å€¤(random_state)",0, 100, 42)
    }


def ui_random_forest_params():
    return {
        "n_estimators": st.slider("æ±ºå®šæœ¨ã®æœ¬æ•° (n_estimators)", 10, 300, 100, step=10),
        "max_depth": st.slider("æœ¨ã®æœ€å¤§æ·±ã• (max_depth)", 1, 20, 5),
        "min_samples_split": st.slider("ãƒãƒ¼ãƒ‰åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° (min_samples_split)", 2, 20, 2),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 100, 42)
    }

def ui_svm_params():
    return {
        "C": st.slider("èª¤å·®ã®è¨±å®¹åº¦ (C)", 0.01, 10.0, 1.0, step=0.01),
        "kernel": st.selectbox("ã‚«ãƒ¼ãƒãƒ«é–¢æ•° (kernel)", ["linear", "rbf", "poly", "sigmoid"], index=1),
        "gamma": st.selectbox("Î³ (gamma)", ["scale", "auto"]),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 100, 42),
        "probability": True  # predict_probaã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚å›ºå®š
    }

def ui_logistic_regression_params():
    return {
        "C": st.slider("æ­£å‰‡åŒ–ã®å¼·ã• (C, å°ã•ã„ã»ã©å¼·ã„)", 0.01, 10.0, 1.0, step=0.01),
        "max_iter": st.slider("æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° (max_iter)", 100, 1000, 200, step=50),
        "solver": st.selectbox("ã‚½ãƒ«ãƒãƒ¼ (solver)", ["lbfgs", "liblinear", "saga", "newton-cg"], index=0),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 1000, 42)
    }


def ui_gradient_boosting_params():
    return {
        "n_estimators": st.slider("æ±ºå®šæœ¨ã®æ•° (n_estimators)", 10, 500, 100, step=10),
        "learning_rate": st.slider("å­¦ç¿’ç‡ (learning_rate)", 0.01, 1.0, 0.1, step=0.01),
        "max_depth": st.slider("å„æœ¨ã®æ·±ã• (max_depth)", 1, 10, 3),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 1000, 42)
    }


def ui_mlp_params():
    return {
        "hidden_layer_sizes": (st.slider("éš ã‚Œå±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°", 5, 200, 10),),
        "activation": st.selectbox("æ´»æ€§åŒ–é–¢æ•° (activation)", ["relu", "tanh", "logistic"], index=0),
        "solver": st.selectbox("æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (solver)", ["adam", "sgd", "lbfgs"], index=0),
        "max_iter": st.slider("æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° (max_iter)", 100, 1000, 500, step=50),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 1000, 42)
    }


def ui_xgboost_params():
    return {
        "n_estimators": st.slider("æ±ºå®šæœ¨ã®æœ¬æ•° (n_estimators)", 10, 500, 100, step=10),
        "learning_rate": st.slider("å­¦ç¿’ç‡ (learning_rate)", 0.01, 0.5, 0.1, step=0.01),
        "max_depth": st.slider("æœ¨ã®æ·±ã• (max_depth)", 1, 10, 3),
        "subsample": st.slider("ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«æ¯”ç‡ (subsample)", 0.5, 1.0, 1.0, step=0.05),
        "colsample_bytree": st.slider("åˆ—ã‚µãƒ³ãƒ—ãƒ«æ¯”ç‡ (colsample_bytree)", 0.5, 1.0, 1.0, step=0.05),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 1000, 42)
    }


def ui_lightgbm_params():
    return {
        "n_estimators": st.slider("æ±ºå®šæœ¨ã®æœ¬æ•° (n_estimators)", 10, 500, 100, step=10),
        "learning_rate": st.slider("å­¦ç¿’ç‡ (learning_rate)", 0.01, 0.5, 0.1, step=0.01),
        "max_depth": st.slider("æœ¨ã®æ·±ã• (max_depth)", 1, 15, -1),  # -1ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆåˆ¶é™ãªã—ï¼‰
        "num_leaves": st.slider("è‘‰ã®æ•° (num_leaves)", 2, 128, 31),
        "random_state": st.slider("ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (random_state)", 0, 1000, 42)
    }


def ui_naive_bayes_params():
    return {"nb_type": st.write("ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºã§ã™ï¼ãƒã‚¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ï¼")}


PRAMS_MACHINE = {
    "k-NN": ui_k_nn_params,
    "æ±ºå®šæœ¨": ui_decision_tree_params,
    "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°":ui_logistic_regression_params,
    "ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚º":ui_naive_bayes_params,
    "ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³":ui_svm_params,
    "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ":ui_random_forest_params,
    "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ(MLP)":ui_mlp_params,
    "å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°":ui_gradient_boosting_params,
    "XGBoost":ui_xgboost_params,
    "LightGBM":ui_lightgbm_params
    }




#################
##ãƒ¡ã‚¤ãƒ³ç”»é¢
################


#ã‚¿ã‚¤ãƒˆãƒ«

st.title("Irisãƒ‡ãƒ¼ã‚¿ã‚’æ•™å¸«ã¨ã—ã¦,å…¥åŠ›ã—ãŸèŠ±ã®åˆ†é¡ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒª")

st.write("è‰²ã€…ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’ã§åˆ†é¡ã‚’ã—ã¾ã™ï¼")





#ä½¿ç”¨ã™ã‚‹æ•™å¸«ã‚ã‚Šå­¦ç¿’æ³•ã‚’é¸æŠ

selected_machine = st.radio("ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã‚’é¸ã‚“ã§ã­", list(CLASSIFYING_MACHINES.keys()))



st.subheader("ãƒã‚¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

params = PRAMS_MACHINE[selected_machine]()   # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿UIè¡¨ç¤ºï¼†å–å¾—

pred_class, proba_dict = CLASSIFYING_MACHINES[selected_machine](params)
st.subheader("åˆ†é¡çµæœ")
st.markdown(f"æ‰‹æ³•ã€Œ{selected_machine}ã€ã«ã‚ˆã‚‹ã¨  \nã‚ãªãŸã®å…¥åŠ›ã—ãŸèŠ±ã®ç¨®é¡ã¯**{pred_class}**ã§ã™ï¼")
st.subheader("ğŸ“Š äºˆæ¸¬ä¿¡é ¼åº¦ï¼ˆå„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ï¼‰")
st.json(proba_dict)


#################
##ãƒ¡ã‚¤ãƒ³ç”»é¢
################
